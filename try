import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Generate synthetic credit score data for illustration purposes
np.random.seed(42)
data_size = 1000

# Features
age = np.random.randint(18, 70, data_size)
income = np.random.uniform(20000, 100000, data_size)
loan_amount = np.random.uniform(5000, 50000, data_size)
previous_delinquency = np.random.choice([0, 1], size=data_size, p=[0.9, 0.1])

# Target variable (1: Approved, 0: Denied)
approval_status = np.random.choice([1, 0], size=data_size, p=[0.7, 0.3])

# Create DataFrame
credit_data = pd.DataFrame({
    'Age': age,
    'Income': income,
    'LoanAmount': loan_amount,
    'PreviousDelinquency': previous_delinquency,
    'ApprovalStatus': approval_status
})

# Display the first few rows of the dataset
print(credit_data.head())

# Separate features and target variable
X = credit_data.drop('ApprovalStatus', axis=1)  # Features
y = credit_data['ApprovalStatus']  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Define the XGBoost model
xgb_model = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)

# Train the XGBoost model
xgb_model.fit(X_train, y_train)

# Define the RandomForest model
rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)

# Train the RandomForest model
rf_model.fit(X_train, y_train)

# Evaluate models on the test set
xgb_y_pred = xgb_model.predict(X_test)
rf_y_pred = rf_model.predict(X_test)
